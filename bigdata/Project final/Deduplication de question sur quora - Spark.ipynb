{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Presentation:\n",
    "    - Spark et pandas sont differents. Pandas met tout en ram tandis que spark ne charge la donnée qu'au moment d'une action et ecrit sur le disque les resultats intermediaire\n",
    "    - On a pas des machines assez grosses pour voir vraiment les limites.\n",
    "    - Si ça rentre pas en ram alors on peut pas utiliser pandas \n",
    "    - Pour pouvoir benchmarker il faudrait faire exactement la même chose dans les 2 et être aussi bon dans l'un que dans l'autre\n",
    "    - Pandas est column-based donc le code peut tres souvent être optimiser (par exemple un apply sur les lignes est vraiment pas performant par rapport a un apply sur les colonnes.\n",
    "    - Spark lui est row-based et une particularité est que si on fait une udf, la jvm doit créer un process python  + serializer la données + etc ce qui entraine un sur coup.\n",
    "    - Pour optimiser le code spark il faut aussi faire attention au cache \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Etude \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "python_path = '/Users/mnannan/.virtualenvs/supelec/bin/python'\n",
    "\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = python_path\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = python_path\n",
    "\n",
    "\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.context import SparkContext, SparkConf\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "sc = SparkContext()\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = './data/train.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importation des données\n",
    "\n",
    "Pour tester les perfomances de lectures nous allons simplement collecter toutes les données ainsi spark devra lire toutes les données (simplement compter le nombre de lignes ne serait pas significatif car spark utiliserait des metadatas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.93 s, sys: 149 ms, total: 2.07 s\n",
      "Wall time: 8.05 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df = spark.read.csv(DATA_PATH, header=True, escape='\"')\n",
    "_ = df.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour le reste de l'etude nous allons mettre la donnée spark en memoire pour ne pas avoir de surcôut lié au fait que spark ne stocke rien en memoire par defaut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = spark.read.csv('./data/train.csv', header=True, escape='\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "404301"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_full.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict = {\n",
    "    1000: df_full.limit(1000),\n",
    "    10000: df_full.limit(10000),\n",
    "    100000: df_full.limit(100000),\n",
    "    400000: df_full,\n",
    "    800000: df_full.union(df_full),\n",
    "    1200000: df_full.union(df_full).union(df_full)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _, df in df_dict.items():\n",
    "    df.cache()\n",
    "    df.distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Séparation des données en un ensemble d'apprentissage et un ensemble de validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "taux_sep = 0.7\n",
    "def split_dataset(df, taux_sep):\n",
    "    return df.randomSplit([taux_sep, 1-taux_sep])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Nettoyage des données\n",
    "\n",
    "Pour le nettoyage des données nous allons:\n",
    "- supprimer les questions vides\n",
    "- retirer les stopwords\n",
    "- passer le text en minuscule\n",
    "- tokenizer les questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "english_stopwords = list(set(stopwords.words(\"english\")))\n",
    "\n",
    "from pyspark.ml.feature import StopWordsRemover, Tokenizer\n",
    "\n",
    "\n",
    "def clean_dataframe(df):\n",
    "    for column in ['question1', 'question2']:\n",
    "        df = df.filter(F.col(column).isNotNull())\n",
    "        df = df.withColumn(column, F.lower(F.col(column)))\n",
    "        tokenizer = Tokenizer(inputCol=column, outputCol=f'{column}_tokenized')\n",
    "        stopwords_remover = StopWordsRemover(inputCol=f'{column}_tokenized', outputCol=f'{column}_tokenized_cleaned', stopWords=english_stopwords)\n",
    "        for task in [tokenizer, stopwords_remover]:\n",
    "            df = task.transform(df)\n",
    "        \n",
    "        df = df.drop(f'{column}_tokenized', column)\n",
    "        df = df.withColumnRenamed(f'{column}_tokenized_cleaned', column)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = {}\n",
    "cleaning_time = {}\n",
    "\n",
    "for key, df in df_dict.items():\n",
    "    st = time.time()\n",
    "    df_cleaned[key] = clean_dataframe(df)\n",
    "    df_cleaned[key].distinct().count()\n",
    "    cleaning_time[key] = time.time() - st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1000: 0.6644432544708252,\n",
       " 10000: 0.5577132701873779,\n",
       " 100000: 1.7603912353515625,\n",
       " 400000: 3.1319870948791504,\n",
       " 800000: 4.577440023422241,\n",
       " 1200000: 6.014577150344849}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaning_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, df in df_cleaned.items():\n",
    "    df.cache()\n",
    "    df.distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.  Représentation des données.\n",
    "\n",
    "Pour représenter nos données (i.e. la description textuelle des produits), pluieurs principes seront utilisés et comparés :\n",
    "\n",
    " + L'approche de représentation d'un document textuel par un sac de mots de type `one_hot_encoding` avec scikit-learn comme expliqué [ici](scikit-learn)\n",
    " + L'approche de représentation d'un document textuel par un sac de mots et une pondération [tf-idf](https://fr.wikipedia.org/wiki/TF-IDF) vue dans les premiers cours. De nombreux modules sont disponibles dans scikit-learn, notamment [ici](https://scikit-learn.org/stable/modules/feature_extraction.html#the-bag-of-words-representation) pour son implémentation.\n",
    " + Une approche de hachage qui est une des techniques utilisées pour le traitement des données massives. Elle consiste à réduire fortement le volume de calculs à faire sur les données en réduisant l'ordre de complexité des calculs à faire par l'exploitation des caractéristiques de similarité des données. Ici aussi, vous pouvez tirer partie des modules existants dans scikit-learn décrits [ici](https://scikit-learn.org/stable/modules/feature_extraction.html#vectorizing-a-large-text-corpus-with-the-hashing-trick).\n",
    " + Une représentation de type word2vec avec la bibliothèque [gensim](https://radimrehurek.com/gensim/).\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Calcul de l'idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_count = 2\n",
    "eps = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_idf(df):\n",
    "    df_idf = df.select('question1')\\\n",
    "    .union(df.select('question2')).distinct()\\\n",
    "    .select(F.explode('question1').alias('word'))\\\n",
    "    .groupBy('word')\\\n",
    "    .agg(F.count('*').alias('idf'))\\\n",
    "    .withColumn(\n",
    "        'idf',\n",
    "        F.when(F.col('idf') < min_count, F.lit(0))\\\n",
    "        .otherwise(1/(F.col('idf')+eps))\n",
    "    )\n",
    "    return df_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_idf_dict = {}\n",
    "idf_dict = {}\n",
    "\n",
    "idf_time = {}\n",
    "\n",
    "for key, df in df_cleaned.items():\n",
    "    st = time.time()\n",
    "    df_idf_dict[key] = compute_idf(df)\n",
    "    idf_dict[key] = { word: idf for word,idf in df_idf_dict[key].collect()}\n",
    "    idf_time[key] = time.time() - st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1000: 0.6151769161224365,\n",
       " 10000: 0.4928293228149414,\n",
       " 100000: 1.2541282176971436,\n",
       " 400000: 2.2709591388702393,\n",
       " 800000: 2.772006034851074,\n",
       " 1200000: 3.2807397842407227}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, df in df_idf_dict.items():\n",
    "    df.cache()\n",
    "    df.distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Common words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_common_words(df):\n",
    "    df = df.withColumn(\n",
    "        'common_words',\n",
    "        2 * F.size(F.array_intersect('question1', 'question2'))/ (F.size('question1') + F.size('question2'))\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_common_words = {}\n",
    "\n",
    "common_words_time = {}\n",
    "\n",
    "for key, df in df_cleaned.items():\n",
    "    st = time.time()\n",
    "    df_common_words[key] = compute_common_words(df)\n",
    "    df_common_words[key].distinct().count()\n",
    "    common_words_time[key] = time.time() - st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1000: 0.15171313285827637,\n",
       " 10000: 0.11482405662536621,\n",
       " 100000: 0.5359230041503906,\n",
       " 400000: 1.7709598541259766,\n",
       " 800000: 1.7045090198516846,\n",
       " 1200000: 2.117926836013794}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_words_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Common words sans udf with join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_common_words(df):\n",
    "    df_words = df.select('id', F.explode('question1').alias('word1'), 'question2')\\\n",
    "    .select('id', 'word1', F.explode('question2').alias('word2'))\n",
    "\n",
    "    shared_words = df_words\\\n",
    "    .filter('word1 = word2')\\\n",
    "    .groupby('id')\\\n",
    "    .agg(F.count('*').alias('common_words'))\n",
    "\n",
    "    df = df.join(shared_words, 'id', 'left')\\\n",
    "    .withColumn(\n",
    "        'common_words',\n",
    "        F.when(F.col('common_words').isNull(), F.lit(0))\\\n",
    "        .otherwise(F.col('common_words'))\n",
    "    )\\\n",
    "    .withColumn(\n",
    "        'common_words',\n",
    "       2*F.col('common_words') / (F.size('question1') + F.size('question2'))\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_common_words = {}\n",
    "\n",
    "common_words_time = {}\n",
    "\n",
    "for key, df in df_cleaned.items():\n",
    "    st = time.time()\n",
    "    df_common_words[key] = compute_common_words(df)\n",
    "    df_common_words[key].distinct().count()\n",
    "    common_words_time[key] = time.time() - st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1000: 0.2672290802001953,\n",
       " 10000: 0.32927918434143066,\n",
       " 100000: 1.5108647346496582,\n",
       " 400000: 2.2074599266052246,\n",
       " 800000: 4.227099180221558,\n",
       " 1200000: 5.615463018417358}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_words_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, df in df_common_words.items():\n",
    "    df.cache()\n",
    "    df.distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Common words avec udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "@F.udf(T.IntegerType())\n",
    "def udf_common_words(question1, question2):\n",
    "    q1words = {}\n",
    "    q2words = {}\n",
    "    for word in question1:\n",
    "        q1words[word] = 1\n",
    "    for word in question2:\n",
    "        q2words[word] = 1\n",
    "    if len(q1words) == 0 or len(q2words) == 0:\n",
    "        # The computer-generated chaff includes a few questions that are nothing but stopwords\n",
    "        return 0\n",
    "    shared_words = [w for w in q1words.keys() if w in q2words]\n",
    "    R = (2 * len(shared_words))/(len(q1words) + len(q2words))\n",
    "    return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_common_words_with_udf(df):\n",
    "    df = df.withColumn(\n",
    "        'common_words',\n",
    "        udf_common_words(F.col('question1'), F.col('question2')))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_common_words_with_udf = {}\n",
    "\n",
    "common_words_with_udf_time = {}\n",
    "\n",
    "for key, df in df_cleaned.items():\n",
    "    st = time.time()\n",
    "    df_common_words_with_udf[key] = compute_common_words_with_udf(df)\n",
    "    df_common_words_with_udf[key].distinct().count()\n",
    "    common_words_with_udf_time[key] = time.time() - st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1000: 0.7200469970703125,\n",
       " 10000: 0.27631497383117676,\n",
       " 100000: 1.1112160682678223,\n",
       " 400000: 2.1631557941436768,\n",
       " 800000: 3.177809953689575,\n",
       " 1200000: 4.694714069366455}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_words_with_udf_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### common words idf weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_common_words_idf_weighted(df):\n",
    "    df_words = df.select('id', F.explode('question1').alias('word1'), 'question2')\\\n",
    "        .select('id', 'word1', F.explode('question2').alias('word2'))\n",
    "\n",
    "    shared_idf_weighted = df_words\\\n",
    "    .filter('word1 = word2')\\\n",
    "    .join(df_idf.withColumnRenamed('word', 'word1') , 'word1', 'left')\\\n",
    "    .groupby('id')\\\n",
    "    .agg(F.sum('idf').alias('common_words_idf_weighted'))\n",
    "\n",
    "    df_question_idf = df.select('question1','qid1').union(df.select('question2', 'qid2')).distinct()\\\n",
    "    .select(F.explode('question1').alias('word'), 'qid1')\\\n",
    "    .join(df_idf, 'word', 'left')\\\n",
    "    .groupby('qid1')\\\n",
    "    .agg(F.sum('idf').alias('question_idf'))\n",
    "    df_question_idf.cache()\n",
    "\n",
    "    df = df.join(shared_idf_weighted, 'id', 'left')\\\n",
    "    .join(\n",
    "        df_question_idf.withColumnRenamed('question_idf', 'question1_idf'),\n",
    "        'qid1',\n",
    "        'left'\n",
    "    )\\\n",
    "    .join(\n",
    "        df_question_idf\\\n",
    "        .withColumnRenamed('question_idf', 'question2_idf')\\\n",
    "        .withColumnRenamed('qid1', 'qid2'),\n",
    "        'qid2',\n",
    "        'left'\n",
    "    )\\\n",
    "    .withColumn(\n",
    "        'common_words_idf_weighted',\n",
    "        F.when(F.col('common_words_idf_weighted').isNull(), F.lit(0))\\\n",
    "        .otherwise(F.col('common_words_idf_weighted'))\n",
    "    )\\\n",
    "    .withColumn(\n",
    "       'common_words_idf_weighted',\n",
    "       2*F.col('common_words_idf_weighted') / (F.col('question1_idf') + F.col('question2_idf'))\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_common_words_idf = {}\n",
    "\n",
    "common_words_idf_time = {}\n",
    "\n",
    "for key, df in df_common_words.items():\n",
    "    df_idf = df_idf_dict[key]\n",
    "    st = time.time()\n",
    "    df_common_words_idf[key] = compute_common_words_idf_weighted(df)\n",
    "    df_common_words_idf[key].distinct().count()\n",
    "    common_words_idf_time[key] = time.time() - st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1000: 1.3751139640808105,\n",
       " 10000: 1.370466947555542,\n",
       " 100000: 3.8508951663970947,\n",
       " 400000: 6.787045240402222,\n",
       " 800000: 8.197191953659058,\n",
       " 1200000: 32.96792387962341}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_words_idf_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, df in df_common_words_idf.items():\n",
    "    df.cache()\n",
    "    df.distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### common words idf weighted avec udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def common_words_idf_weighted(question1, question2, idf):\n",
    "    common_words_weighted = 0\n",
    "    questions_weights = 0\n",
    "    for word in question1:\n",
    "        idf_weight = idf[word]\n",
    "        if word in question2:\n",
    "            common_words_weighted += idf_weight\n",
    "        questions_weights += idf_weight\n",
    "    for word in question2:\n",
    "        questions_weights += idf[word]\n",
    "    if questions_weights > 0:\n",
    "        return 2 * common_words_weighted / questions_weights\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_common_words_idf_weighted_with_udf(df):\n",
    "    @F.udf(T.IntegerType())\n",
    "    def udf_common_words_idf_weighted(question1, question2):\n",
    "        return common_words_idf_weighted(question1, question2)\n",
    "\n",
    "    df.withColumn(\n",
    "        'common_words_idf_weighted',\n",
    "        udf_common_words_idf_weighted(F.col('question1'), F.col('question2'))\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_common_words_idf_with_udf = {}\n",
    "\n",
    "common_words_idf_time_with_udf = {}\n",
    "\n",
    "for key, df in df_common_words.items():\n",
    "    st = time.time()\n",
    "    df_common_words_idf_with_udf[key] = compute_common_words_idf_weighted_with_udf(df)\n",
    "    df_common_words_idf_with_udf[key].distinct().count()\n",
    "    common_words_idf_time_with_udf[key] = time.time() - st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'full': 0.302523136138916,\n",
       " '1000': 0.048006296157836914,\n",
       " '10000': 0.06462216377258301,\n",
       " '100000': 0.22162413597106934}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_words_idf_time_with_udf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Creation du train et du test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dict = {}\n",
    "test_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, df in df_common_words_idf.items():\n",
    "    df = df.filter('is_duplicate is not null')\n",
    "\n",
    "    df = df.withColumnRenamed('is_duplicate', 'label')\\\n",
    "    .withColumn('label', F.col('label').cast('int'))\n",
    "\n",
    "    selected_columns = ['common_words', 'common_words_idf_weighted']\n",
    "\n",
    "    for column in selected_columns:\n",
    "        df = df.filter(F.col(column).isNotNull())\n",
    "        df = df.withColumn(column, F.col(column).cast('double'))\n",
    "\n",
    "    train, test = split_dataset(df, taux_sep)\n",
    "    assembler = VectorAssembler(inputCols=selected_columns, outputCol=\"features\")\n",
    "\n",
    "    pipeline = Pipeline(stages=[assembler])\n",
    "    pipelineModel = pipeline.fit(df)\n",
    "    train_dict[key] = pipelineModel.transform(train)\n",
    "    test_dict[key] = pipelineModel.transform(test)\n",
    "    train_dict[key].cache().distinct().count()\n",
    "    test_dict[key].cache().distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.  Apprentissage et performance\n",
    "\n",
    "Pour tester les perfomances des modeles nous allons entrainer une regression logistic, un random forest et un arbre de decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression Logistique "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr_dict = {}\n",
    "lrModel_dict = {}\n",
    "\n",
    "lr_time = {}\n",
    "\n",
    "for key, train in train_dict.items():\n",
    "    st = time.time()\n",
    "    # Create initial LogisticRegression model\n",
    "    lr_dict[key] = LogisticRegression(labelCol=\"label\", featuresCol=\"features\")\n",
    "\n",
    "    # Train model with Training Data\n",
    "    lrModel_dict[key] = lr_dict[key].fit(train)\n",
    "    lr_time[key] = time.time() - st\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1000: 0.5644731521606445,\n",
       " 10000: 0.6493039131164551,\n",
       " 100000: 0.9996500015258789,\n",
       " 400000: 2.5949480533599854,\n",
       " 800000: 3.173823118209839,\n",
       " 1200000: 5.683064222335815}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 0.6846846846846847\n",
      "10000 0.6808873720136519\n",
      "100000 0.6767363542396052\n",
      "400000 0.6689562934464938\n",
      "800000 0.6683974335200089\n",
      "1200000 0.669596254959296\n"
     ]
    }
   ],
   "source": [
    "for key, test in test_dict.items():\n",
    "    predictions = lrModel_dict[key].transform(test)\n",
    "    evaluator = MulticlassClassificationEvaluator(labelCol='label', predictionCol='prediction', metricName='accuracy')\n",
    "    print(key, evaluator.evaluate(predictions))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Arbres de décision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "\n",
    "dt_dict = {}\n",
    "dtModel_dict = {}\n",
    "\n",
    "dt_time = {}\n",
    "\n",
    "for key, train in train_dict.items():\n",
    "    st = time.time()\n",
    "    # Create initial LogisticRegression model\n",
    "    dt_dict[key] = DecisionTreeClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
    "\n",
    "    # Train model with Training Data\n",
    "    dtModel_dict[key] = dt_dict[key].fit(train)\n",
    "    dt_time[key] = time.time() - st\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1000: 1.2671706676483154,\n",
       " 10000: 1.3924741744995117,\n",
       " 100000: 0.9892888069152832,\n",
       " 400000: 2.2401349544525146,\n",
       " 800000: 2.6141581535339355,\n",
       " 1200000: 4.613914966583252}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 0.7477477477477478\n",
      "10000 0.7133105802047781\n",
      "100000 0.7169484178586909\n",
      "400000 0.7100870168254545\n",
      "800000 0.7121116591872814\n",
      "1200000 0.7086556017736776\n"
     ]
    }
   ],
   "source": [
    "for key, test in test_dict.items():\n",
    "    predictions = dtModel_dict[key].transform(test)\n",
    "    evaluator = MulticlassClassificationEvaluator(labelCol='label', predictionCol='prediction', metricName='accuracy')\n",
    "    print(key, evaluator.evaluate(predictions))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "rf_dict = {}\n",
    "rfModel_dict = {}\n",
    "\n",
    "rf_time = {}\n",
    "\n",
    "for key, train in train_dict.items():\n",
    "    st = time.time()\n",
    "    # Create initial LogisticRegression model\n",
    "    rf_dict[key] = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
    "\n",
    "    # Train model with Training Data\n",
    "    rfModel_dict[key] = rf_dict[key].fit(train)\n",
    "    rf_time[key] = time.time() - st\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1000: 0.7193012237548828,\n",
       " 10000: 0.7812149524688721,\n",
       " 100000: 2.272455930709839,\n",
       " 400000: 5.494510889053345,\n",
       " 800000: 7.683736801147461,\n",
       " 1200000: 10.278573036193848}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full 0.7311377191640953\n",
      "1000 0.735593220338983\n",
      "10000 0.7229958599924727\n",
      "100000 0.7327012159814708\n"
     ]
    }
   ],
   "source": [
    "for key, test in test_dict.items():\n",
    "    predictions = rfModel_dict[key].transform(test)\n",
    "    evaluator = MulticlassClassificationEvaluator(labelCol='label', predictionCol='prediction', metricName='accuracy')\n",
    "    print(key, evaluator.evaluate(predictions))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "supelec",
   "language": "python",
   "name": "supelec"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
